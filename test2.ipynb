{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 model...\n",
      "GPT-2 model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "model_id = \"gpt2\"  # GPT-2 is smaller, fully open, and good for prototyping\n",
    "print(\"Loading GPT-2 model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "print(\"GPT-2 model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract preferences from user's message\n",
    "def extract_preferences(message, carsDF):\n",
    "    preferences = {}\n",
    "\n",
    "    # Check if the user is asking for available models or car types\n",
    "    if \"model\" in message.lower() or \"type\" in message.lower() or \"offer\" in message.lower():\n",
    "        preferences['ask_for_models'] = True\n",
    "\n",
    "    # New or Used\n",
    "    if \"new\" in message.lower() or \"used\" in message.lower():\n",
    "        preferences['car_type'] = 'New' if 'new' in message.lower() else 'Used'\n",
    "\n",
    "    # Year\n",
    "    year = [int(word) for word in message.split() if word.isdigit() and len(word) == 4]\n",
    "    if year:\n",
    "        preferences['year'] = year[0]\n",
    "\n",
    "    # Make\n",
    "    makes = carsDF['Make'].str.lower().unique()\n",
    "    if any(make in message.lower() for make in makes):\n",
    "        preferences['make'] = next(make.title() for make in makes if make in message.lower())\n",
    "\n",
    "    # Model\n",
    "    models = carsDF['Model'].str.lower().unique()\n",
    "    if any(model in message.lower() for model in models):\n",
    "        preferences['model'] = next(model.title() for model in models if model in message.lower())\n",
    "\n",
    "    # Fast car (determine based on engine displacement or high-performance description)\n",
    "    if \"fast\" in message.lower() or \"high performance\" in message.lower():\n",
    "        preferences['fast_car'] = True\n",
    "\n",
    "    return preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get matching cars based on preferences\n",
    "def get_matching_cars(preferences, carsDF):\n",
    "    if 'ask_for_models' in preferences:\n",
    "        return list_available_models(carsDF)\n",
    "\n",
    "    filtered_cars = carsDF\n",
    "\n",
    "    # Apply each preference as a filter\n",
    "    if 'car_type' in preferences:\n",
    "        filtered_cars = filtered_cars[filtered_cars['Type'].str.lower() == preferences['car_type'].lower()]\n",
    "\n",
    "    if 'year' in preferences:\n",
    "        filtered_cars = filtered_cars[filtered_cars['Year'] == preferences['year']]\n",
    "\n",
    "    if 'make' in preferences:\n",
    "        filtered_cars = filtered_cars[filtered_cars['Make'].str.lower() == preferences['make'].lower()]\n",
    "\n",
    "    if 'model' in preferences:\n",
    "        filtered_cars = filtered_cars[filtered_cars['Model'].str.lower() == preferences['model'].lower()]\n",
    "\n",
    "    if 'fast_car' in preferences:\n",
    "        # Assume fast cars have large engine displacement or high horsepower descriptions\n",
    "        filtered_cars = filtered_cars[filtered_cars['EngineDisplacement'].str.replace(\" L\", \"\").astype(float) > 3.0]\n",
    "\n",
    "    # Return the top 3 matching cars\n",
    "    if not filtered_cars.empty:\n",
    "        return filtered_cars[['Make', 'Model', 'Year', 'VIN', 'SellingPrice', 'Miles', 'Fuel_Type', 'PassengerCapacity']].head(3)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate LLM response\n",
    "def generate_llm_response(context):\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list available models\n",
    "def list_available_models(carsDF):\n",
    "    makes_and_models = carsDF[['Make', 'Model']].drop_duplicates()\n",
    "    model_list = \"\"\n",
    "    for index, row in makes_and_models.iterrows():\n",
    "        model_list += f\"- {row['Make']} {row['Model']}\\n\"\n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 17:57:17.018 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-23 17:57:17.118 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/rachelsu/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-11-23 17:57:17.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-23 17:57:17.119 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-23 17:57:17.120 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-23 17:57:17.120 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-23 17:57:17.121 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-23 17:57:17.122 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Streamlit UI\n",
    "st.title(\"Matador Car Match Chatbot\")\n",
    "\n",
    "# Sidebar for CSV file upload\n",
    "csv_data = st.sidebar.file_uploader(\"Upload vehicles.csv\", type=\"csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if csv_data is not None:\n",
    "    # Handle uploaded CSV file with temporary file\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\") as tmp_file:\n",
    "        tmp_file.write(csv_data.getvalue())\n",
    "        tmp_file_path = tmp_file.name\n",
    "\n",
    "    # Load CSV data using pandas directly\n",
    "    carsDF = pd.read_csv(tmp_file_path)\n",
    "\n",
    "    # Chat UI\n",
    "    if 'user' not in st.session_state:\n",
    "        st.session_state['user'] = [\"Hi there! Please start describing which type of car you are looking for.\"]\n",
    "\n",
    "    if 'assistant' not in st.session_state:\n",
    "        st.session_state['assistant'] = [\"Hello, I am Matador Chatbot! How can I help you find your car today?\"]\n",
    "\n",
    "    container = st.container()\n",
    "\n",
    "    with container:\n",
    "        with st.form(key='car_form', clear_on_submit=True):\n",
    "            user_input = st.text_input(\"\", placeholder=\"Enter your car preferences...\", key='input')\n",
    "            submit = st.form_submit_button(label='Submit')\n",
    "\n",
    "        if submit:\n",
    "            # Append user message\n",
    "            st.session_state['user'].append(user_input)\n",
    "\n",
    "            # Extract preferences and find matching cars\n",
    "            preferences = extract_preferences(user_input, carsDF)\n",
    "            if 'ask_for_models' in preferences:\n",
    "                # Provide a list of available models if user is asking for types/models of cars\n",
    "                llm_response = \"Here are some of the car models we offer:\\n\\n\" + list_available_models(carsDF)\n",
    "            else:\n",
    "                matching_cars = get_matching_cars(preferences, carsDF)\n",
    "\n",
    "                # If preferences are not detailed enough, ask follow-up questions\n",
    "                if not preferences:\n",
    "                    llm_response = \"Could you please provide more details? For example, do you have a budget or a specific brand in mind?\"\n",
    "                elif matching_cars is None:\n",
    "                    llm_response = \"I couldn't find any cars matching your preferences. Could you refine your criteria?\"\n",
    "                else:\n",
    "                    # Create a prettier output for the car results\n",
    "                    llm_response = \"Here are the top 3 cars that match your preferences:\"\n",
    "                    for index, car in matching_cars.iterrows():\n",
    "                        llm_response += f\"\\n\\n- **{car['Year']} {car['Make']} {car['Model']}**\\n\"\n",
    "                        llm_response += f\"  - VIN: {car['VIN']}\\n\"\n",
    "                        llm_response += f\"  - Selling Price: ${car['SellingPrice']:,.2f}\\n\"\n",
    "                        llm_response += f\"  - Mileage: {car['Miles']} miles\\n\"\n",
    "                        llm_response += f\"  - Fuel Type: {car['Fuel_Type']}\\n\"\n",
    "                        llm_response += f\"  - Passenger Capacity: {car['PassengerCapacity']}\"\n",
    "\n",
    "            # Generate LLM-based response for more dynamic interaction\n",
    "            context = f\"User: {user_input}\\nAssistant: {llm_response}\"\n",
    "            llm_generated_response = generate_llm_response(context)\n",
    "\n",
    "            # Append both responses\n",
    "            st.session_state['assistant'].append(llm_generated_response)\n",
    "\n",
    "    # Display chat messages\n",
    "    if st.session_state['assistant']:\n",
    "        for i in range(len(st.session_state['assistant'])):\n",
    "            # Display user message\n",
    "            st.write(f\"**User:** {st.session_state['user'][i]}\")\n",
    "            # Display assistant message\n",
    "            st.markdown(f\"**Assistant:** {st.session_state['assistant'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#streamlit run /Users/rachelsu/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
